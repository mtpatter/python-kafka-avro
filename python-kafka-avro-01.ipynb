{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python to send Avro data with Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for using Python with the confluent-kafka Python client (which uses [librdkafka](https://github.com/edenhill/librdkafka)) to send Avro data in Kafka.\n",
    "\n",
    "Here is my github repofor this code and notebook:  https://github.com/mtpatter/python-kafka-avro. It has a Dockerfile for my public Docker image also available on Docker hub at https://hub.docker.com/r/mtpatter/python-kafka-avro/.\n",
    "\n",
    "This version uses the following:\n",
    "\n",
    "1. Kafka version 0.10 from the Confluent docker repo\n",
    "2. zookeeper from wurstmeister's docker repo\n",
    "3. my own docker image with the new Python client from Kafka (confluent-kafka) and avro-python3\n",
    "4. simple producer and consumer scripts modified from [cuongbangoc's upstream repo](https://github.com/cuongbangoc/python-kafka-avro)\n",
    "\n",
    "Not sure if this is the best way to do these things, but it works for me currently as a start.  I am also not sure why I get an assertion error in the consumer, so I've caught that exception and ignored it.\n",
    "\n",
    "I used these very helpful resources:\n",
    "\n",
    "https://activisiongamescience.github.io/2016/06/15/Kafka-Client-Benchmarking/  \n",
    "https://github.com/cuongbangoc/python-kafka-avro  \n",
    "https://gist.github.com/jakekdodd/e7ee38fd945818d86eb4  \n",
    "https://sonnguyen.ws/simple-example-python-kafka-avro/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run this demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Kafka broker and zookeeper with docker-compose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"pythonkafkaavro_default\" with the default driver\n",
      "Creating pythonkafkaavro_zookeeper_1\n",
      "Creating pythonkafkaavro_kafka_1\n"
     ]
    }
   ],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a producer going to produce some Avro data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!docker run -it -P --network=host --volume=${PWD}:/home mtpatter/python-kafka-avro python producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a consumer going to listen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% my-topic [0] at offset 0 with key None:\n",
      "{'favorite_number': 6, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 1 with key None:\n",
      "{'favorite_number': 2, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 2 with key None:\n",
      "{'favorite_number': 7, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 3 with key None:\n",
      "{'favorite_number': 3, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 4 with key None:\n",
      "{'favorite_number': 8, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 5 with key None:\n",
      "{'favorite_number': 2, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 6 with key None:\n",
      "{'favorite_number': 10, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 7 with key None:\n",
      "{'favorite_number': 4, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 8 with key None:\n",
      "{'favorite_number': 2, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] at offset 9 with key None:\n",
      "{'favorite_number': 4, 'favorite_color': '111', 'name': '123'}\n",
      "% my-topic [0] reached end at offset 10\n"
     ]
    }
   ],
   "source": [
    "!docker run -it -P --network=host --volume=${PWD}:/home mtpatter/python-kafka-avro python -u consumer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output appears in consumer.  Remember to clean up by shutting down producer and consumer and run \"docker-compose down\" to shut down Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from confluent_kafka import Producer\r\n",
      "import avro.schema\r\n",
      "import avro.io\r\n",
      "import io\r\n",
      "import random\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "\r\n",
      "    conf = {'bootstrap.servers': 'localhost:9092'}\r\n",
      "    producer = Producer(**conf)\r\n",
      "\r\n",
      "    # Kafka topic\r\n",
      "    topic = \"my-topic\"\r\n",
      "\r\n",
      "    # Path to user.avsc avro schema\r\n",
      "    schema_path = \"user.avsc\"\r\n",
      "    schema = avro.schema.Parse(open(schema_path).read())\r\n",
      "\r\n",
      "    for i in range(10):\r\n",
      "        writer = avro.io.DatumWriter(schema)\r\n",
      "        bytes_writer = io.BytesIO()\r\n",
      "        encoder = avro.io.BinaryEncoder(bytes_writer)\r\n",
      "        writer.write({\"name\": \"123\",\r\n",
      "                      \"favorite_color\": \"111\",\r\n",
      "                      \"favorite_number\": random.randint(0, 10)}, encoder)\r\n",
      "        raw_bytes = bytes_writer.getvalue()\r\n",
      "        producer.produce(topic, raw_bytes)\r\n",
      "    producer.flush()\r\n"
     ]
    }
   ],
   "source": [
    "!cat producer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from confluent_kafka import Consumer, KafkaException, KafkaError\r\n",
      "import avro.schema\r\n",
      "import avro.io\r\n",
      "import io\r\n",
      "import sys\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "\r\n",
      "    # To consume messages\r\n",
      "    conf = {'bootstrap.servers': 'localhost:9092',\r\n",
      "            'group.id': 'my_group',\r\n",
      "            'default.topic.config': {'auto.offset.reset': 'smallest'}}\r\n",
      "    consumer = Consumer(**conf)\r\n",
      "    topic = consumer.subscribe(['my-topic'])\r\n",
      "\r\n",
      "    schema_path = \"user.avsc\"\r\n",
      "    schema = avro.schema.Parse(open(schema_path).read())\r\n",
      "\r\n",
      "    try:\r\n",
      "        running = True\r\n",
      "        while running:\r\n",
      "            msg = consumer.poll(timeout=60000)\r\n",
      "\r\n",
      "            if msg is None:\r\n",
      "                continue\r\n",
      "            if msg.error():\r\n",
      "                if msg.error().code() == KafkaError._PARTITION_EOF:\r\n",
      "                    sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\r\n",
      "                                     (msg.topic(), msg.partition(),\r\n",
      "                                      msg.offset()))\r\n",
      "                elif msg.error():\r\n",
      "                    raise KafkaException(msg.error())\r\n",
      "            else:\r\n",
      "                sys.stderr.write('%% %s [%d] at offset %d with key %s:\\n' %\r\n",
      "                                 (msg.topic(), msg.partition(), msg.offset(),\r\n",
      "                                  str(msg.key())))\r\n",
      "\r\n",
      "            message = msg.value()\r\n",
      "            bytes_reader = io.BytesIO(message)\r\n",
      "            decoder = avro.io.BinaryDecoder(bytes_reader)\r\n",
      "            reader = avro.io.DatumReader(schema)\r\n",
      "            try:\r\n",
      "                decoded_msg = reader.read(decoder)\r\n",
      "                print(decoded_msg)\r\n",
      "                sys.stdout.flush()\r\n",
      "            except AssertionError:\r\n",
      "                continue\r\n",
      "\r\n",
      "    except KeyboardInterrupt:\r\n",
      "        sys.stderr.write('%% Aborted by user\\n')\r\n"
     ]
    }
   ],
   "source": [
    "!cat consumer.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
